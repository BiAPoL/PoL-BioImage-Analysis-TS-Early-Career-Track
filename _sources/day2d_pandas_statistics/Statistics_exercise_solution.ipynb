{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Solutions for exercises - Improving performance\n",
                "\n",
                "first we load the data and define functions so that we are at the same state as in the exercise notebook"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Solutions for exercises - Improving performance\n",
                "\n",
                "first we load the data and define functions so that we are at the same state as in the exercise notebook"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "from scipy.io import loadmat\n",
                "\n",
                "data = loadmat('../../data/microtubule_tracking/summary.mat')\n",
                "data\n",
                "df = pd.DataFrame()\n",
                "microtubule_id = 0\n",
                "for i, amppnp_concentration in enumerate(data['concentrations']):\n",
                "    all_frame_to_frame_velocities = data['allSpeeds'][i][0]\n",
                "    for j in range(all_frame_to_frame_velocities.shape[1]):\n",
                "        df = pd.concat([df, pd.DataFrame({'AMPPNP concentration': amppnp_concentration[0], 'file name': data['fileNames'][i][0][0], 'microtubule ID': microtubule_id, 'frame-to-frame microtubule tip velocity': all_frame_to_frame_velocities[:, j]})], ignore_index=True)\n",
                "        microtubule_id += 1\n",
                "df = df.dropna()\n",
                "controls = df[df['AMPPNP concentration']==0]\n",
                "# extract data for the individual controls\n",
                "control1 = controls[controls['file name'] == '2015-12-12_Run_1_AMPPNP_row_1_ATP_1_107.nd2']\n",
                "control2 = controls[controls['file name'] == '2015-12-12_Run_1_AMPPNP_row_1_ATP_2_108.nd2']\n",
                "\n",
                "def get_confidence_interval_by_bootstrapping(measurements:pd.DataFrame, bootstrap_size: int = 1000, alpha: float = 0.05):\n",
                "    '''Estimate confidence intervals for data in a pandas dataframe by bootstrapping'''\n",
                "    bootstrap_samples = np.empty((bootstrap_size,))\n",
                "    for i in range(bootstrap_size):\n",
                "        resample_index = np.random.choice(measurements.index,size=measurements.shape[0])\n",
                "        bootstrap_samples[i] = np.median(measurements['frame-to-frame microtubule tip velocity'][resample_index])\n",
                "    lower_bound = alpha * 100 / 2\n",
                "    upper_bound = 100 - lower_bound\n",
                "    return np.percentile(bootstrap_samples,[lower_bound, upper_bound])\n",
                "\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "57313617",
            "metadata": {},
            "source": [
                "Depending on the size of your data and the complexity of the function you use to calculate $\\hat\\theta$, bootstrapping can take quite some time. The computational effort required for bootsrapping was the reason, why bootstrapping was not immediately adopted by the scientific community at the time it was invented by Bradley Efron in 1979. But nowadays, with powerful computers widely available, that is not a problem any more. Considering that your experiment probably took days or even weeks, it is o.k. if the data analysis takes a few minutes or even hours. Consider it a matter of respect for your data to analyze it properly.\n",
                "\n",
                "However, in our case, there is actually something we can do about the time the calculations are taking. Pandas is nice and all, but performance is not its strong suit. Numpy is much faster - particularly in combination with just-in-time compilation facilitated by numba. \n",
                "\n",
                "### Exercise 1\n",
                "\n",
                "Let's convert our data to numpy arrays and try  `get_confidence_interval_by_bootstrapping` again."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "2e1c6b37",
            "metadata": {},
            "outputs": [
                {
                    "ename": "AttributeError",
                    "evalue": "'numpy.ndarray' object has no attribute 'index'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn [3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m numpy_control1 \u001b[39m=\u001b[39m control1[\u001b[39m'\u001b[39m\u001b[39mframe-to-frame microtubule tip velocity\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues\n\u001b[1;32m      2\u001b[0m numpy_control2 \u001b[39m=\u001b[39m control2[\u001b[39m'\u001b[39m\u001b[39mframe-to-frame microtubule tip velocity\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues\n\u001b[0;32m----> 4\u001b[0m get_confidence_interval_by_bootstrapping(numpy_control1)\n",
                        "Cell \u001b[0;32mIn [2], line 24\u001b[0m, in \u001b[0;36mget_confidence_interval_by_bootstrapping\u001b[0;34m(measurements, bootstrap_size, alpha)\u001b[0m\n\u001b[1;32m     22\u001b[0m bootstrap_samples \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty((bootstrap_size,))\n\u001b[1;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(bootstrap_size):\n\u001b[0;32m---> 24\u001b[0m     resample_index \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(measurements\u001b[39m.\u001b[39;49mindex,size\u001b[39m=\u001b[39mmeasurements\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[1;32m     25\u001b[0m     bootstrap_samples[i] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmedian(measurements[\u001b[39m'\u001b[39m\u001b[39mframe-to-frame microtubule tip velocity\u001b[39m\u001b[39m'\u001b[39m][resample_index])\n\u001b[1;32m     26\u001b[0m lower_bound \u001b[39m=\u001b[39m alpha \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n",
                        "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'index'"
                    ]
                }
            ],
            "source": [
                "numpy_control1 = control1['frame-to-frame microtubule tip velocity'].values\n",
                "numpy_control2 = control2['frame-to-frame microtubule tip velocity'].values\n",
                "\n",
                "get_confidence_interval_by_bootstrapping(numpy_control1)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "6e266ebe",
            "metadata": {},
            "source": [
                "So our function expects pandas tables. In order for it to work with numpy arrays, we need to rewrite it.\n",
                "\n",
                "While we are at it, let's also make our function more modular, so we don't need to rewrite the whole thing every time we change the data type.\n",
                "\n",
                "### Exercise 2\n",
                "\n",
                "Let's so make our function more modular, so we don't need to rewrite the whole thing every time we change the data type.\n",
                "Split the function into three parts:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "0ebf7099",
            "metadata": {},
            "outputs": [],
            "source": [
                "#Turn this:\n",
                "def get_confidence_interval_by_bootstrapping(measurements:pd.DataFrame, bootstrap_size: int = 1000, alpha: float = 0.05):\n",
                "    '''Estimate confidence intervals for data in a pandas dataframe by bootstrapping'''\n",
                "    bootstrap_samples = np.empty((bootstrap_size,))\n",
                "    for i in range(bootstrap_size):\n",
                "        resample_index = np.random.choice(measurements.index,size=measurements.shape[0])\n",
                "        bootstrap_samples[i] = np.median(measurements['frame-to-frame microtubule tip velocity'][resample_index])\n",
                "    lower_bound = alpha * 100 / 2\n",
                "    upper_bound = 100 - lower_bound\n",
                "    return np.percentile(bootstrap_samples,[lower_bound, upper_bound])\n",
                "\n",
                "#into three functions:\n",
                "import numba\n",
                "@numba.njit #this decorator enables the numba just in time compiler for the function defined in the next line\n",
                "def draw_bootstrap_sample(data: np.ndarray):\n",
                "    \"\"\"Draw a bootstrap sample from a 1D data set.\"\"\"\n",
                "    return np.random.choice(data, size=data.shape[0])\n",
                "\n",
                "@numba.njit\n",
                "def bootstrap_median(data: np.ndarray, bootstrap_size=1000):\n",
                "    \"\"\"Aply a function to many boostrap samples.\"\"\"\n",
                "    bootstrap_replicates = np.empty(bootstrap_size)\n",
                "    for i in range(bootstrap_size):\n",
                "        bootstrap_replicates[i] = np.median(draw_bootstrap_sample(data))\n",
                "    return bootstrap_replicates\n",
                "\n",
                "@numba.njit\n",
                "def get_confidence_interval(boostrap_replicates: np.ndarray, alpha: float = 0.05):\n",
                "    \"\"\"Calculate the confidence interval with conficence level alpha from bootstrap replicates\"\"\"\n",
                "    lower_bound = alpha * 100 / 2\n",
                "    upper_bound = 100 - lower_bound\n",
                "    return np.percentile(boostrap_replicates,[lower_bound, upper_bound])   \n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise 3:\n",
                "\n",
                "apply your new functions to the data to calculate confidence intervals. Make sure that the results are the same as before"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "37a1719f",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Confidence interval 1: [774.37708725 784.96674312]\n",
                        "Confidence interval 2: [778.7187312  792.65464497]\n"
                    ]
                }
            ],
            "source": [
                "# Fixed seed for reproducibility\n",
                "np.random.seed(42)\n",
                "\n",
                "# convert pandas tables to numpy arrays\n",
                "numpy_control1 = control1['frame-to-frame microtubule tip velocity'].values\n",
                "numpy_control2 = control2['frame-to-frame microtubule tip velocity'].values\n",
                "\n",
                "confidence_interval1 = get_confidence_interval(bootstrap_median(numpy_control1))\n",
                "confidence_interval2 = get_confidence_interval(bootstrap_median(numpy_control2))\n",
                "print('Confidence interval 1: {}\\nConfidence interval 2: {}'.format(confidence_interval1, confidence_interval2))\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "1f65c13a",
            "metadata": {},
            "source": [
                "### Exercise 4:\n",
                "\n",
                "Let's benchmark our two functions:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "15595e9b",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "pandas:\n",
                        "212 ms ± 1.34 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
                        "numpy/numba:\n",
                        "47 ms ± 198 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "print('pandas:')\n",
                "%timeit get_confidence_interval_by_bootstrapping(control1)\n",
                "\n",
                "print('numpy/numba:')\n",
                "%timeit get_confidence_interval(bootstrap_median(numpy_control1))\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "d44aa4fa",
            "metadata": {},
            "source": [
                "A factor of four - this could actually save us quite a bit of time for larger datasets or more complicated target functions."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.9.13 ('dbnapari-arm64')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 17:00:33) \n[Clang 13.0.1 ]"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "f13a533d7f6b525f1b9e27d74dd8bd7ea06c4c95ae7aaf0187c7426193b5690e"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
